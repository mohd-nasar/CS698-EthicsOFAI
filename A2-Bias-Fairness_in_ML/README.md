# Making Fair Predictive Models - Mitigating Bias

## Problem Statement  

Predictive models in education can help identify students at risk of dropping out, enabling timely interventions. However, such models often inherit or amplify biases from real-world data, leading to unfair outcomes across demographic groups.  

This project uses the **UCI “Predict Students’ Dropout and Academic Success” dataset** to build classification models that not only predict student outcomes (**Dropout, Enrolled, Graduate**) with high accuracy but also address **fairness** concerns.  

### Objectives  
- **Bias Identification**: Detect potential sources of unfairness in the dataset.  
- **Model Development**: Train classifiers (e.g., XGBoost, Random Forest) to predict outcomes.  
- **Fairness Evaluation**: Assess performance across demographic groups using fairness metrics.  
- **Bias Mitigation**: Apply strategies like feature elimination, reweighting, and calibration.  
- **Ethical Reflection**: Discuss trade-offs between accuracy and fairness in real-world impact.  

The goal is to create models that are not only **accurate** but also **equitable**, supporting fair decision-making in education.  

